{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPsVVzNhtFWr/toAZ5XpZsL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guttulaswathi/ml/blob/main/Assignment6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.build a multiple linear regression model on a dataset 50_startups"
      ],
      "metadata": {
        "id": "JHteGxJ-37rN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gz6ilodGvHWE",
        "outputId": "63d1615a-3796-4c6c-b960-eace6ff4c70c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 82010363.04430099\n",
            "R² Score: 0.8987266414328637\n",
            "Coefficients: [ 8.05630064e-01 -6.87878823e-02  2.98554429e-02  9.38793006e+02\n",
            "  6.98775997e+00]\n",
            "Intercept: 54028.03959364581\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "data = pd.read_csv('/content/50_Startups.csv')\n",
        "data = pd.get_dummies(data, columns=['State'], drop_first=True)\n",
        "X = data.drop('Profit', axis=1)\n",
        "y = data['Profit']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "print(f\"R² Score: {r2}\")\n",
        "print(\"Coefficients:\", model.coef_)\n",
        "print(\"Intercept:\", model.intercept_)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.apply L1 regularization on the created simple and multiple linear regression. what is your observation"
      ],
      "metadata": {
        "id": "l526ujFA5JV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "lasso_model = Lasso(alpha=1.0, random_state=42)\n",
        "\n",
        "lasso_model.fit(X_train, y_train)\n",
        "y_pred_lasso = lasso_model.predict(X_test)\n",
        "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
        "r2_lasso = r2_score(y_test, y_pred_lasso)\n",
        "\n",
        "print(\"Lasso Regression (Multiple Linear Regression) Results:\")\n",
        "print(f\"Mean Squared Error: {mse_lasso}\")\n",
        "print(f\"R² Score: {r2_lasso}\")\n",
        "print(\"Coefficients:\", lasso_model.coef_)\n",
        "print(\"Intercept:\", lasso_model.intercept_)\n",
        "print(\"\\nObservation:\")\n",
        "print(\"Some coefficients might be zero if Lasso identified them as less important.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOPZD1q-0eQP",
        "outputId": "db3b641a-6eb7-42a0-c1ca-54b8ecc2de9c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lasso Regression (Multiple Linear Regression) Results:\n",
            "Mean Squared Error: 82004202.15414938\n",
            "R² Score: 0.8987342494230525\n",
            "Coefficients: [ 8.05623603e-01 -6.87794946e-02  2.98665959e-02  9.30428825e+02\n",
            "  0.00000000e+00]\n",
            "Intercept: 54030.09459385941\n",
            "\n",
            "Observation:\n",
            "Some coefficients might be zero if Lasso identified them as less important.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Observations:\n",
        "\n",
        "# Lasso vs. Ordinary Least Squares (OLS):\n",
        "\n",
        "In OLS, all coefficients are retained, regardless of their importance.\n",
        "\n",
        "In Lasso, irrelevant or less important features might have their coefficients shrunk to zero, effectively performing feature selection.\n",
        "\n",
        "# Impact of Regularization:\n",
        "\n",
        "Increasing the alpha parameter in Lasso strengthens the penalty, leading to more coefficients being reduced to zero.\n",
        "\n",
        "This reduces model complexity but may also lead to underfitting if the penalty is too strong.\n",
        "\n",
        "# Interpretability:\n",
        "\n",
        "Lasso helps in identifying the most relevant features for predicting the target variable.\n",
        "\n",
        "Coefficients set to zero indicate features that do not contribute significantly."
      ],
      "metadata": {
        "id": "d6efW1Ft2e_t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.apply L2 regularization on the created simple and multiple linear regression. what is your observation"
      ],
      "metadata": {
        "id": "SQAy1_wv4gKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "ridge_model = Ridge(alpha=1.0, random_state=42)\n",
        "ridge_model.fit(X_train, y_train)\n",
        "y_pred_ridge = ridge_model.predict(X_test)\n",
        "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
        "r2_ridge = r2_score(y_test, y_pred_ridge)\n",
        "\n",
        "print(\"Ridge Regression (Multiple Linear Regression) Results:\")\n",
        "print(f\"Mean Squared Error: {mse_ridge}\")\n",
        "print(f\"R² Score: {r2_ridge}\")\n",
        "print(\"Coefficients:\", ridge_model.coef_)\n",
        "print(\"Intercept:\", ridge_model.intercept_)\n",
        "print(\"\\nObservation:\")\n",
        "print(\"Coefficients are shrunk towards zero but not exactly zero.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whyjsKee04kn",
        "outputId": "596d8c35-44c7-472c-ccbd-4d97325004ea"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ridge Regression (Multiple Linear Regression) Results:\n",
            "Mean Squared Error: 81887773.66036233\n",
            "R² Score: 0.8988780252113923\n",
            "Coefficients: [ 8.05466887e-01 -6.86656370e-02  3.00321810e-02  8.12091828e+02\n",
            " -5.37609242e+01]\n",
            "Intercept: 54048.330539008864\n",
            "\n",
            "Observation:\n",
            "Coefficients are shrunk towards zero but not exactly zero.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Observations:\n",
        "## Ridge vs. Lasso:\n",
        "\n",
        "Ridge shrinks all coefficients proportionally, retaining all features but reducing their influence.\n",
        "\n",
        "Unlike Lasso, Ridge does not perform feature selection (i.e., coefficients are not set to exactly zero).\n",
        "\n",
        "### Impact of Regularization:\n",
        "\n",
        "Increasing the alpha parameter strengthens the penalty, leading to smaller coefficient values.\n",
        "\n",
        "This helps in reducing multicollinearity and overfitting, but too much regularization can lead to underfitting.\n",
        "\n",
        "### Model Complexity:\n",
        "\n",
        "Ridge maintains all features in the model, making it suitable when all predictors are expected to have some contribution to the target.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GVlz7Vn81yEi"
      }
    }
  ]
}